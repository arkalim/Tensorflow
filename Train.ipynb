{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arkalim/Tensorflow/blob/master/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEJIVjAwtFPs",
        "colab_type": "text"
      },
      "source": [
        "# Train DeepLab V3\n",
        "\n",
        "## Clone the repository for necessary helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFaSVkRdQwy4",
        "colab_type": "code",
        "outputId": "66a4da4d-55b3-4961-eee0-757057651a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/sthalles/deeplab_v3.git\n",
        "os.listdir('/content/deeplab_v3')\n",
        "os.chdir('deeplab_v3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplab_v3'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "Receiving objects:   0% (1/267)   \rReceiving objects:   1% (3/267)   \rReceiving objects:   2% (6/267)   \rReceiving objects:   3% (9/267)   \rReceiving objects:   4% (11/267)   \rReceiving objects:   5% (14/267)   \rReceiving objects:   6% (17/267)   \rReceiving objects:   7% (19/267)   \rReceiving objects:   8% (22/267)   \rReceiving objects:   9% (25/267)   \rReceiving objects:  10% (27/267)   \rReceiving objects:  11% (30/267)   \rReceiving objects:  12% (33/267)   \rReceiving objects:  13% (35/267)   \rReceiving objects:  14% (38/267)   \rReceiving objects:  15% (41/267)   \rReceiving objects:  16% (43/267)   \rReceiving objects:  17% (46/267)   \rReceiving objects:  18% (49/267)   \rReceiving objects:  19% (51/267)   \rReceiving objects:  20% (54/267)   \rReceiving objects:  21% (57/267)   \rReceiving objects:  22% (59/267)   \rReceiving objects:  23% (62/267)   \rReceiving objects:  24% (65/267)   \rReceiving objects:  25% (67/267)   \rReceiving objects:  26% (70/267)   \rReceiving objects:  27% (73/267)   \rReceiving objects:  28% (75/267)   \rReceiving objects:  29% (78/267)   \rReceiving objects:  30% (81/267)   \rReceiving objects:  31% (83/267)   \rReceiving objects:  32% (86/267)   \rReceiving objects:  33% (89/267)   \rReceiving objects:  34% (91/267)   \rReceiving objects:  35% (94/267)   \rReceiving objects:  36% (97/267)   \rReceiving objects:  37% (99/267)   \rReceiving objects:  38% (102/267)   \rReceiving objects:  39% (105/267)   \rReceiving objects:  40% (107/267)   \rReceiving objects:  41% (110/267)   \rReceiving objects:  42% (113/267)   \rReceiving objects:  43% (115/267)   \rReceiving objects:  44% (118/267)   \rremote: Total 267 (delta 0), reused 0 (delta 0), pack-reused 267\u001b[K\n",
            "Receiving objects:  45% (121/267)   \rReceiving objects:  46% (123/267)   \rReceiving objects:  47% (126/267)   \rReceiving objects:  48% (129/267)   \rReceiving objects:  49% (131/267)   \rReceiving objects:  50% (134/267)   \rReceiving objects:  51% (137/267)   \rReceiving objects:  52% (139/267)   \rReceiving objects:  53% (142/267)   \rReceiving objects:  54% (145/267)   \rReceiving objects:  55% (147/267)   \rReceiving objects:  56% (150/267)   \rReceiving objects:  57% (153/267)   \rReceiving objects:  58% (155/267)   \rReceiving objects:  59% (158/267)   \rReceiving objects:  60% (161/267)   \rReceiving objects:  61% (163/267)   \rReceiving objects:  62% (166/267)   \rReceiving objects:  63% (169/267)   \rReceiving objects:  64% (171/267)   \rReceiving objects:  65% (174/267)   \rReceiving objects:  66% (177/267)   \rReceiving objects:  67% (179/267)   \rReceiving objects:  68% (182/267)   \rReceiving objects:  69% (185/267)   \rReceiving objects:  70% (187/267)   \rReceiving objects:  71% (190/267)   \rReceiving objects:  72% (193/267)   \rReceiving objects:  73% (195/267)   \rReceiving objects:  74% (198/267)   \rReceiving objects:  75% (201/267)   \rReceiving objects:  76% (203/267)   \rReceiving objects:  77% (206/267)   \rReceiving objects:  78% (209/267)   \rReceiving objects:  79% (211/267)   \rReceiving objects:  80% (214/267)   \rReceiving objects:  81% (217/267)   \rReceiving objects:  82% (219/267)   \rReceiving objects:  83% (222/267)   \rReceiving objects:  84% (225/267)   \rReceiving objects:  85% (227/267)   \rReceiving objects:  86% (230/267)   \rReceiving objects:  87% (233/267)   \rReceiving objects:  88% (235/267)   \rReceiving objects:  89% (238/267)   \rReceiving objects:  90% (241/267)   \rReceiving objects:  91% (243/267)   \rReceiving objects:  92% (246/267)   \rReceiving objects:  93% (249/267)   \rReceiving objects:  94% (251/267)   \rReceiving objects:  95% (254/267)   \rReceiving objects:  96% (257/267)   \rReceiving objects:  97% (259/267)   \rReceiving objects:  98% (262/267)   \rReceiving objects:  99% (265/267)   \rReceiving objects: 100% (267/267)   \rReceiving objects: 100% (267/267), 571.29 KiB | 10.78 MiB/s, done.\n",
            "Resolving deltas:   0% (0/136)   \rResolving deltas:   1% (2/136)   \rResolving deltas:   5% (8/136)   \rResolving deltas:   7% (10/136)   \rResolving deltas:   8% (11/136)   \rResolving deltas:   9% (13/136)   \rResolving deltas:  14% (20/136)   \rResolving deltas:  16% (22/136)   \rResolving deltas:  18% (25/136)   \rResolving deltas:  19% (27/136)   \rResolving deltas:  22% (31/136)   \rResolving deltas:  24% (33/136)   \rResolving deltas:  27% (38/136)   \rResolving deltas:  55% (76/136)   \rResolving deltas:  57% (78/136)   \rResolving deltas:  63% (87/136)   \rResolving deltas:  67% (92/136)   \rResolving deltas:  70% (96/136)   \rResolving deltas:  77% (105/136)   \rResolving deltas:  86% (118/136)   \rResolving deltas:  88% (121/136)   \rResolving deltas:  92% (126/136)   \rResolving deltas:  93% (127/136)   \rResolving deltas:  94% (129/136)   \rResolving deltas:  98% (134/136)   \rResolving deltas:  99% (135/136)   \rResolving deltas: 100% (136/136)   \rResolving deltas: 100% (136/136), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D67uBlW001It",
        "colab_type": "code",
        "outputId": "46461be9-3125-4061-831a-75fc0ebb7c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "slim = tf.contrib.slim\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import network\n",
        "from preprocessing.read_data import download_resnet_checkpoint_if_necessary, tf_record_parser, \\\n",
        "    rescale_image_and_annotation_by_factor, scale_image_with_crop_padding, \\\n",
        "    random_flip_image_and_annotation, distort_randomly_image_color\n",
        "from preprocessing import training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 04:50:20.745241 139633178437504 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D32irFxY027J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_norm_epsilon = 1e-5            # batch norm epsilon for batch normalization\n",
        "batch_norm_decay = 0.9997            # batch norm decay for batch normalization\n",
        "number_of_classes = 21               # number of classes to predict \n",
        "l2_regularizer = 0.0001              # l2 regularization parameter\n",
        "multi_grid = [1,2,4]                 # Spatial Pyramid Pooling rates\n",
        "output_stride = 16                   # Output Stride\n",
        "crop_size = 513                      # Image size to feed in the network\n",
        "resnet_model = 'resnet_v2_50'        # Resnet Model [choices: \"resnet_v2_50\", \"resnet_v2_101\", \"resnet_v2_152\", \"resnet_v2_200\"]\n",
        "batch_size = 8                       # Batch size \n",
        "img_shape = (513,513,3)\n",
        "\n",
        "num_train_examples = 8498\n",
        "num_valid_examples = 2857\n",
        "\n",
        "classes = [\"background\",\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv monitor\", \"Unknown\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vt01DP10Bdn",
        "colab_type": "code",
        "outputId": "fd11e931-5f9e-44f8-ed92-91acd6a408c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.listdir('/content/tfrecords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.tfrecords', 'validation.tfrecords']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3YN9PYg1k-D",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data pipeline from tf record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFdeJw2k1g0_",
        "colab_type": "code",
        "outputId": "3fa1a40e-c065-4407-bd8a-b84ed44fed40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "LOG_FOLDER = '/content/checkpoints'\n",
        "TRAIN_DATASET_DIR=\"/content/tfrecords\"\n",
        "TRAIN_FILE = 'train.tfrecords'\n",
        "VALIDATION_FILE = 'validation.tfrecords'\n",
        "\n",
        "# Train Dataset\n",
        "training_filenames = [os.path.join(TRAIN_DATASET_DIR,TRAIN_FILE)]\n",
        "training_dataset = tf.data.TFRecordDataset(training_filenames)\n",
        "training_dataset = training_dataset.map(tf_record_parser)\n",
        "training_dataset = training_dataset.map(rescale_image_and_annotation_by_factor)\n",
        "training_dataset = training_dataset.map(distort_randomly_image_color)\n",
        "training_dataset = training_dataset.map(lambda image, annotation, image_shape: scale_image_with_crop_padding(image, annotation, image_shape, crop_size))\n",
        "training_dataset = training_dataset.map(random_flip_image_and_annotation)  # Parse the record into tensors.\n",
        "training_dataset = training_dataset.repeat()  # number of epochs\n",
        "training_dataset = training_dataset.shuffle(buffer_size=500)\n",
        "training_dataset = training_dataset.batch(batch_size)\n",
        "\n",
        "# Validation Dataset\n",
        "validation_filenames = [os.path.join(TRAIN_DATASET_DIR,VALIDATION_FILE)]\n",
        "validation_dataset = tf.data.TFRecordDataset(validation_filenames)\n",
        "validation_dataset = validation_dataset.map(tf_record_parser)  # Parse the record into tensors.\n",
        "validation_dataset = validation_dataset.map(lambda image, annotation, image_shape: scale_image_with_crop_padding(image, annotation, image_shape, crop_size))\n",
        "validation_dataset = validation_dataset.shuffle(buffer_size=100)\n",
        "validation_dataset = validation_dataset.batch(batch_size)\n",
        "\n",
        "# A feedable iterator is defined by a handle placeholder and its structure.\n",
        "handle = tf.placeholder(tf.string, shape=[])\n",
        "\n",
        "iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)\n",
        "batch_images_tf, batch_labels_tf, _ = iterator.get_next()\n",
        "\n",
        "# You can use feedable iterators with a variety of different kinds of iterator\n",
        "# (such as one-shot and initializable iterators).\n",
        "train_iterator = training_dataset.make_initializable_iterator()\n",
        "valid_iterator = validation_dataset.make_initializable_iterator()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 04:50:20.906049 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/preprocessing/read_data.py:115: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0619 04:50:20.907540 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/preprocessing/read_data.py:121: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0619 04:50:20.938275 139633178437504 deprecation.py:323] From /content/deeplab_v3/preprocessing/read_data.py:132: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0619 04:50:20.940353 139633178437504 deprecation.py:323] From /content/deeplab_v3/preprocessing/read_data.py:134: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0619 04:50:20.967399 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/preprocessing/read_data.py:56: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0619 04:50:20.981494 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/preprocessing/read_data.py:62: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0619 04:50:21.160007 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/preprocessing/read_data.py:98: The name tf.image.resize_image_with_crop_or_pad is deprecated. Please use tf.image.resize_with_crop_or_pad instead.\n",
            "\n",
            "W0619 04:50:23.126173 139633178437504 deprecation.py:323] From <ipython-input-5-b58cb132a311>:29: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
            "W0619 04:50:23.127605 139633178437504 deprecation.py:323] From <ipython-input-5-b58cb132a311>:29: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "W0619 04:50:23.138788 139633178437504 deprecation.py:323] From <ipython-input-5-b58cb132a311>:34: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3fCx_1jAbHh",
        "colab_type": "text"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dQVKr1o8WAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "slim = tf.contrib.slim\n",
        "from resnet import resnet_v2, resnet_utils\n",
        "\n",
        "# ImageNet mean statistics\n",
        "_R_MEAN = 123.68\n",
        "_G_MEAN = 116.78\n",
        "_B_MEAN = 103.94\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def atrous_spatial_pyramid_pooling(net, scope, depth=256, reuse=None):\n",
        "    \"\"\"\n",
        "    ASPP consists of (a) one 1×1 convolution and three 3×3 convolutions with rates = (6, 12, 18) when output stride = 16\n",
        "    (all with 256 filters and batch normalization), and (b) the image-level features as described in https://arxiv.org/abs/1706.05587\n",
        "    :param net: tensor of shape [BATCH_SIZE, WIDTH, HEIGHT, DEPTH]\n",
        "    :param scope: scope name of the aspp layer\n",
        "    :return: network layer with aspp applyed to it.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.variable_scope(scope, reuse=reuse):\n",
        "        feature_map_size = tf.shape(net)\n",
        "\n",
        "        # apply global average pooling\n",
        "        image_level_features = tf.reduce_mean(net, [1, 2], name='image_level_global_pool', keepdims=True)\n",
        "        \n",
        "        image_level_features = slim.conv2d(image_level_features, depth, [1, 1], scope=\"image_level_conv_1x1\", activation_fn=None)\n",
        "        \n",
        "        image_level_features = tf.image.resize_bilinear(image_level_features, (feature_map_size[1], feature_map_size[2]))\n",
        "\n",
        "        at_pool1x1 = slim.conv2d(net, depth, [1, 1], scope=\"conv_1x1_0\", activation_fn=None)\n",
        "\n",
        "        at_pool3x3_1 = slim.conv2d(net, depth, [3, 3], scope=\"conv_3x3_1\", rate=6, activation_fn=None)\n",
        "\n",
        "        at_pool3x3_2 = slim.conv2d(net, depth, [3, 3], scope=\"conv_3x3_2\", rate=12, activation_fn=None)\n",
        "\n",
        "        at_pool3x3_3 = slim.conv2d(net, depth, [3, 3], scope=\"conv_3x3_3\", rate=18, activation_fn=None)\n",
        "\n",
        "        net = tf.concat((image_level_features, at_pool1x1, at_pool3x3_1, at_pool3x3_2, at_pool3x3_3), axis=3, name=\"concat\")\n",
        "        \n",
        "        net = slim.conv2d(net, depth, [1, 1], scope=\"conv_1x1_output\", activation_fn=None)\n",
        "        \n",
        "        return net\n",
        "\n",
        "\n",
        "def deeplab_v3(inputs, l2_regularizer, batch_norm_decay, batch_norm_epsilon, resnet_model, number_of_classes, output_stride, is_training, reuse):\n",
        "\n",
        "    # mean subtraction normalization\n",
        "    inputs = inputs - [_R_MEAN, _G_MEAN, _B_MEAN]\n",
        "\n",
        "    # inputs has shape - Original: [batch_size, 513, 513, 3]\n",
        "    with slim.arg_scope(resnet_utils.resnet_arg_scope(l2_regularizer, is_training, batch_norm_decay, batch_norm_epsilon)):\n",
        "        \n",
        "        resnet = getattr(resnet_v2, resnet_model)\n",
        "        \n",
        "        _, end_points = resnet(inputs, \n",
        "                               number_of_classes,\n",
        "                               is_training=is_training,\n",
        "                               global_pool=False,\n",
        "                               spatial_squeeze=False,\n",
        "                               output_stride=output_stride,\n",
        "                               reuse=reuse)\n",
        "\n",
        "        with tf.variable_scope(\"DeepLab_v3\", reuse = reuse):\n",
        "\n",
        "            # get block 4 feature outputs\n",
        "            net = end_points[resnet_model + '/block4']\n",
        "\n",
        "            net = atrous_spatial_pyramid_pooling(net, scope = \"ASPP_layer\", depth=256, reuse=reuse)\n",
        "\n",
        "            net = slim.conv2d(net, number_of_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='logits')\n",
        "\n",
        "            size = tf.shape(inputs)[1:3]\n",
        "            # resize the output logits to match the labels dimensions\n",
        "            net = tf.image.resize_bilinear(net, size)\n",
        "            \n",
        "            return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyYa05f8jXXz",
        "colab_type": "text"
      },
      "source": [
        "# Download the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq2Cwej5jZkc",
        "colab_type": "code",
        "outputId": "9cb5e6f3-975d-451d-e18c-f5af4f0877c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "resnet_checkpoints_path = \"./resnet/checkpoints/\"\n",
        "download_resnet_checkpoint_if_necessary(resnet_checkpoints_path, resnet_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resnet: resnet_v2_50 successfully downloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHy7wPHynLsD",
        "colab_type": "text"
      },
      "source": [
        "# Define the loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWdrRUcenKmW",
        "colab_type": "code",
        "outputId": "0e0a523b-8d14-493b-ba7c-1a5c79076320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# define the class labels\n",
        "class_labels = [v for v in range((number_of_classes+1))]\n",
        "class_labels[-1] = 255\n",
        "\n",
        "is_training_tf = tf.placeholder(tf.bool, shape=[])\n",
        "\n",
        "logits_tf = tf.cond(is_training_tf, true_fn = lambda: deeplab_v3(batch_images_tf, l2_regularizer, batch_norm_decay, batch_norm_epsilon, resnet_model, number_of_classes, output_stride, is_training=True, reuse=False),\n",
        "                                   false_fn = lambda: deeplab_v3(batch_images_tf, l2_regularizer, batch_norm_decay, batch_norm_epsilon, resnet_model, number_of_classes, output_stride, is_training=False, reuse=True))\n",
        "\n",
        "# get valid logits and labels (factor the 255 padded mask out for cross entropy)\n",
        "valid_labels_batch_tf, valid_logits_batch_tf = training.get_valid_logits_and_labels(annotation_batch_tensor=batch_labels_tf, \n",
        "                                                                                    logits_batch_tensor=logits_tf, \n",
        "                                                                                    class_labels=class_labels)\n",
        "\n",
        "cross_entropies = tf.nn.softmax_cross_entropy_with_logits_v2(logits=valid_logits_batch_tf,\n",
        "                                                             labels=valid_labels_batch_tf)\n",
        "\n",
        "loss = tf.reduce_mean(cross_entropies)\n",
        "\n",
        "# prediction_tensor is the argmax of logits\n",
        "predictions_tf = tf.argmax(logits_tf, axis=3)\n",
        "\n",
        "# Define the function for Mean IOU\n",
        "mean_iou, update_op = tf.contrib.metrics.streaming_mean_iou(tf.argmax(valid_logits_batch_tf, axis=1),\n",
        "                                                        tf.argmax(valid_labels_batch_tf, axis=1),\n",
        "                                                        num_classes=number_of_classes) \n",
        "\n",
        "\n",
        "with tf.variable_scope(\"optimizer_vars\"):\n",
        "    global_step = tf.Variable(0, trainable = False)\n",
        "    optimizer_1 = tf.train.AdamOptimizer(learning_rate = 0.00001).minimize(loss)\n",
        "    optimizer_2 = tf.train.AdamOptimizer(learning_rate = 0.000001).minimize(loss)\n",
        "    optimizer_3 = tf.train.AdamOptimizer(learning_rate = 0.0000001).minimize(loss)\n",
        "    \n",
        "optimizers = [optimizer_1, optimizer_2, optimizer_3]\n",
        "epochs = [7,10,10]    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 04:50:48.236351 139633178437504 deprecation_wrapper.py:119] From /content/deeplab_v3/resnet/resnet_v2.py:184: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0619 04:50:54.097642 139633178437504 deprecation.py:323] From /content/deeplab_v3/preprocessing/training.py:123: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0619 04:50:54.234465 139633178437504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:1179: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XMHA-I9aKI",
        "colab_type": "text"
      },
      "source": [
        "## Function to overwrite the console output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umlAb4z99ZTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_overwrite(step, total_step, loss, mean_iou, operation):\n",
        "    sys.stdout.write('\\r')\n",
        "    \n",
        "    if operation == 'train':\n",
        "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f  Mean IOU: %.4f\" % (step, total_step, loss, mean_iou))\n",
        "        \n",
        "    else:\n",
        "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f  Mean IOU: %.4f\" % (step, total_step, loss, mean_iou))\n",
        "        \n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhGZwrbqIRPX",
        "colab_type": "text"
      },
      "source": [
        "## Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBH94d3AIVGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    \n",
        "    train_steps = int(np.ceil(num_train_examples / float(batch_size)))\n",
        "    valid_steps = int(np.ceil(num_valid_examples / float(batch_size)))\n",
        "\n",
        "\n",
        "    for i, optimizer in enumerate(optimizers):\n",
        "\n",
        "            print('##################### Optimization: {} #####################'.format(i+1))\n",
        "\n",
        "            for epoch in range(epochs[i]):\n",
        "\n",
        "                loss_train = 0\n",
        "                loss_valid = 0\n",
        "\n",
        "                mean_iou_train = 0\n",
        "                mean_iou_valid = 0\n",
        "\n",
        "                running_loss = 0\n",
        "                running_mean_iou = 0\n",
        "\n",
        "                for step in range(1,train_steps):\n",
        "\n",
        "                    # Train our model on the batch of data\n",
        "                    loss_train_step, mean_iou_train_step, _, _ = sess.run([loss,mean_iou, update_op, optimizer], feed_dict = {handle: train_handle, is_training_tf: True})\n",
        "\n",
        "                    loss_train += loss_train_step\n",
        "                    mean_iou_train += mean_iou_train_step\n",
        "\n",
        "                    running_loss = loss_train/step\n",
        "                    running_mean_iou = mean_iou_train/step\n",
        "\n",
        "                    print_overwrite(step, train_steps, running_loss, running_mean_iou, 'train')\n",
        "\n",
        "                for step in range(1,valid_steps):\n",
        "\n",
        "                    loss_valid_step, mean_iou_valid_step, _ = sess.run([loss,mean_iou, update_op], feed_dict = {handle: valid_handle, is_training_tf: True})\n",
        "\n",
        "                    loss_valid += loss_valid_step\n",
        "                    mean_iou_valid += mean_iou_valid_step\n",
        "\n",
        "                    running_loss = loss_valid/step\n",
        "                    running_mean_iou = mean_iou_valid/step\n",
        "\n",
        "                    print_overwrite(step, valid_steps, running_loss, running_mean_iou, 'valid')\n",
        "\n",
        "                loss_train /= train_steps    \n",
        "                loss_valid /= valid_steps\n",
        "                mean_iou_train /= train_steps\n",
        "                mean_iou_valid /= valid_steps\n",
        "\n",
        "                print('\\n-----------------------------------------------------------------')\n",
        "                print (\"Epoch: \" + str(epoch+1) + \", Train Loss: \" + \"{:.4f}\".format(loss_train) + \", Train IOU: \" + \"{:.4f}\".format(mean_iou_train))\n",
        "                print (\"          Valid Loss: \" + \"{:.4f}\".format(loss_valid) + \", Valid IOU: \" + \"{:.4f}\".format(mean_iou_valid))\n",
        "                print('-----------------------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C58Wt9uOAjnU",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06n2AUr_-rX",
        "colab_type": "code",
        "outputId": "5a2b61d2-aeb8-446e-9267-01526443a814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Declare the variable to restore\n",
        "variables_to_restore = slim.get_variables_to_restore(exclude=[resnet_model + \"/logits\", \"optimizer_vars\",\"DeepLab_v3/ASPP_layer\", \"DeepLab_v3/logits\"])\n",
        "\n",
        "# Add an operation to restore all the variables.\n",
        "restorer = tf.train.Saver(variables_to_restore)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Create a saver.\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # load resnet checkpoints\n",
        "    restorer.restore(sess, \"./resnet/checkpoints/\" + resnet_model + \".ckpt\")\n",
        "    print(\"Model checkpoints for \" + resnet_model + \" restored!\")\n",
        "\n",
        "    train_handle = sess.run(train_iterator.string_handle())\n",
        "    valid_handle = sess.run(valid_iterator.string_handle())\n",
        "\n",
        "    sess.run(train_iterator.initializer)\n",
        "    sess.run(valid_iterator.initializer)\n",
        "\n",
        "    train()\n",
        "\n",
        "    # Save the variables \n",
        "    save_path = saver.save(sess, LOG_FOLDER + \"/train\" + \"/model.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0619 04:51:24.455890 139633178437504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model checkpoints for resnet_v2_50 restored!\n",
            "##################### Optimization: 1 #####################\n",
            "Valid Steps: 357/358  Loss: 0.4936  Mean IOU: 0.3693\n",
            "-----------------------------------------------------------------\n",
            "Epoch: 1, Train Loss: 0.8976, Train IOU: 0.2485\n",
            "          Valid Loss: 0.4922, Valid IOU: 0.3683\n",
            "-----------------------------------------------------------------\n",
            "Train Steps: 2/1063  Loss: 0.6077  Mean IOU: 0.3811"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}